{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import neighbors\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "from PIL import Image, ImageDraw\n",
    "import face_recognition\n",
    "from face_recognition.face_recognition_cli import image_files_in_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n",
    "    \"\"\"\n",
    "    Trains a k-nearest neighbors classifier for face recognition.\n",
    "\n",
    "    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n",
    "\n",
    "     (View in source code to see train_dir example tree structure)\n",
    "\n",
    "     Structure:\n",
    "        <train_dir>/\n",
    "        ├── <person1>/\n",
    "        │   ├── <somename1>.jpeg\n",
    "        │   ├── <somename2>.jpeg\n",
    "        │   ├── ...\n",
    "        ├── <person2>/\n",
    "        │   ├── <somename1>.jpeg\n",
    "        │   └── <somename2>.jpeg\n",
    "        └── ...\n",
    "\n",
    "    :param model_save_path: (optional) path to save model on disk\n",
    "    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n",
    "    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n",
    "    :param verbose: verbosity of training\n",
    "    :return: returns knn classifier that was trained on the given data.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Loop through each person in the training set\n",
    "    for class_dir in os.listdir(train_dir):\n",
    "        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n",
    "            continue\n",
    "\n",
    "        # Loop through each training image for the current person\n",
    "        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n",
    "            image = face_recognition.load_image_file(img_path)\n",
    "            face_bounding_boxes = face_recognition.face_locations(image)\n",
    "\n",
    "            if len(face_bounding_boxes) != 1:\n",
    "                # If there are no people (or too many people) in a training image, skip the image.\n",
    "                if verbose:\n",
    "                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n",
    "            else:\n",
    "                # Add face encoding for current image to the training set\n",
    "                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n",
    "                y.append(class_dir)\n",
    "\n",
    "    # Determine how many neighbors to use for weighting in the KNN classifier\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int(round(math.sqrt(len(X))))\n",
    "        if verbose:\n",
    "            print(\"Chose n_neighbors automatically:\", n_neighbors)\n",
    "\n",
    "    # Create and train the KNN classifier\n",
    "    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n",
    "    knn_clf.fit(X, y)\n",
    "\n",
    "    # Save the trained KNN classifier\n",
    "    if model_save_path is not None:\n",
    "        with open(model_save_path, 'wb') as f:\n",
    "            pickle.dump(knn_clf, f)\n",
    "\n",
    "    return knn_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN classifier...\n",
      "Training complete!\n",
      "59.94 1920 1080\n",
      "Writing frame 1 / 581\n",
      "Writing frame 2 / 581\n",
      "Writing frame 3 / 581\n",
      "Writing frame 4 / 581\n",
      "Writing frame 5 / 581\n",
      "Writing frame 6 / 581\n",
      "Writing frame 7 / 581\n",
      "Writing frame 8 / 581\n",
      "Writing frame 9 / 581\n",
      "Writing frame 10 / 581\n",
      "Writing frame 11 / 581\n",
      "Writing frame 12 / 581\n",
      "Writing frame 13 / 581\n",
      "Writing frame 14 / 581\n",
      "Writing frame 15 / 581\n",
      "Writing frame 16 / 581\n",
      "Writing frame 17 / 581\n",
      "Writing frame 18 / 581\n",
      "Writing frame 19 / 581\n",
      "Writing frame 20 / 581\n",
      "Writing frame 21 / 581\n",
      "Writing frame 22 / 581\n",
      "Writing frame 23 / 581\n",
      "Writing frame 24 / 581\n",
      "Writing frame 25 / 581\n",
      "Writing frame 26 / 581\n",
      "Writing frame 27 / 581\n",
      "Writing frame 28 / 581\n",
      "Writing frame 29 / 581\n",
      "Writing frame 30 / 581\n",
      "Writing frame 31 / 581\n",
      "Writing frame 32 / 581\n",
      "Writing frame 33 / 581\n",
      "Writing frame 34 / 581\n",
      "Writing frame 35 / 581\n",
      "Writing frame 36 / 581\n",
      "Writing frame 37 / 581\n",
      "Writing frame 38 / 581\n",
      "Writing frame 39 / 581\n",
      "Writing frame 40 / 581\n",
      "Writing frame 41 / 581\n",
      "Writing frame 42 / 581\n",
      "Writing frame 43 / 581\n",
      "Writing frame 44 / 581\n",
      "Writing frame 45 / 581\n",
      "Writing frame 46 / 581\n",
      "Writing frame 47 / 581\n",
      "Writing frame 48 / 581\n",
      "Writing frame 49 / 581\n",
      "Writing frame 50 / 581\n",
      "Writing frame 51 / 581\n",
      "Writing frame 52 / 581\n",
      "Writing frame 53 / 581\n",
      "Writing frame 54 / 581\n",
      "Writing frame 55 / 581\n",
      "Writing frame 56 / 581\n",
      "Writing frame 57 / 581\n",
      "Writing frame 58 / 581\n",
      "Writing frame 59 / 581\n",
      "Writing frame 60 / 581\n",
      "Writing frame 61 / 581\n",
      "Writing frame 62 / 581\n",
      "Writing frame 63 / 581\n",
      "Writing frame 64 / 581\n",
      "Writing frame 65 / 581\n",
      "Writing frame 66 / 581\n",
      "Writing frame 67 / 581\n",
      "Writing frame 68 / 581\n",
      "Writing frame 69 / 581\n",
      "Writing frame 70 / 581\n",
      "Writing frame 71 / 581\n",
      "Writing frame 72 / 581\n",
      "Writing frame 73 / 581\n",
      "Writing frame 74 / 581\n",
      "Writing frame 75 / 581\n",
      "Writing frame 76 / 581\n",
      "Writing frame 77 / 581\n",
      "Writing frame 78 / 581\n",
      "Writing frame 79 / 581\n",
      "Writing frame 80 / 581\n",
      "Writing frame 81 / 581\n",
      "Writing frame 82 / 581\n",
      "Writing frame 83 / 581\n",
      "Writing frame 84 / 581\n",
      "Writing frame 85 / 581\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# STEP 1: Train the KNN classifier and save it to disk\n",
    "# Once the model is trained and saved, you can skip this step next time.\n",
    "print(\"Training KNN classifier...\")\n",
    "knn_clf = train(\"alx_examples/train\", model_save_path=\"alx_examples/trained_knn_model.clf\", n_neighbors=2)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# STEP 2: Using the trained classifier, make predictions for unknown images\n",
    "\n",
    "\n",
    "# Open the input movie file\n",
    "input_movie = cv2.VideoCapture(\"alx_examples/test/DSC_0273.mp4\")\n",
    "\n",
    "## Lấy ra số lượng khung hình chụp được từ video\n",
    "length = int(input_movie.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "## Lấy ra kích thước khung hình\n",
    "height = int(input_movie.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(input_movie.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "## Lấy ra chiều dài của video theo giây\n",
    "input_movie.set(cv2.CAP_PROP_POS_AVI_RATIO,1)\n",
    "seconds = input_movie.get(cv2.CAP_PROP_POS_MSEC) / 1000\n",
    "input_movie.set(cv2.CAP_PROP_POS_AVI_RATIO,0)\n",
    "\n",
    "## Tính Số lượng khung hình ghi được trong 1 giây\n",
    "fps = round(length/seconds, 2)\n",
    "print(fps, width, height )\n",
    "\n",
    "# Create an output movie file (make sure resolution/frame rate matches input video!)\n",
    "## FourCC is a 4-byte code used to specify the video codec\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "## cv2.VideoWriter(output_file_name, FourCC code, number_of_frames_per_second(fps), frame size)\n",
    "output_movie = cv2.VideoWriter('alx_examples/result/DSC_0273_output.avi', fourcc, float(fps), (int(width), int(height)))\n",
    "\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "frame_number = 0\n",
    "distance_threshold = 0.6\n",
    "\n",
    "## List để lưu lại tất cả giá trị face_locations của tất cả các khung hình\n",
    "pre_locations = []\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ## Hàm .read() trả về 1 giá trị bool 'ret' = True/False cho biết khung hình có được đọc hay không và 'frame' là array các pixel của khung hình đó\n",
    "    ret, frame = input_movie.read()\n",
    "    frame_number += 1\n",
    "\n",
    "    # Quit when the input video file ends\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_frame = frame[:, :, ::-1]\n",
    "\n",
    "    # Find all the faces and face encodings in the current frame of video\n",
    "    ## face_locations: tìm kiếm các khuôn mặt có trong ảnh và trả về vị trí các bounding boxes\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    ## Lưu lại tất cả các face_locations vào list pre_locations\n",
    "    pre_locations.append(face_locations)\n",
    "    \n",
    "    ## Trường hợp không tìm được khuôn măt ở khung hình\n",
    "    if face_locations == []: \n",
    "        ## Tìm trong list khuôn mặt tìm được trong 10 khung hình gần nhất trước đó để gán cho khung hình này\n",
    "        for i, location in enumerate(reversed(pre_locations)):\n",
    "            if location != [] and i < 10:\n",
    "                face_locations = location\n",
    "                break\n",
    "         \n",
    "    #print(face_locations)\n",
    "    ## face_encodings: trả về mỗi array 128-dimension encoding cho mỗi khuôn mặt, trong trường hợp đã biết face_locations\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "    \n",
    "    if(face_encodings == []):\n",
    "        face_encodings.append(np.zeros((128,)))\n",
    "    \n",
    "    # Use the KNN model to find the best matches for the test face\n",
    "    closest_distances = knn_clf.kneighbors(face_encodings, n_neighbors=1)\n",
    "    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(face_locations))]\n",
    "\n",
    "\n",
    "    # Predict classes and remove classifications that aren't within the threshold\n",
    "    predictions = [(pred, loc) if rec else ('Unknown', loc) for pred, loc, rec in zip(knn_clf.predict(face_encodings), face_locations, are_matches)]\n",
    "    \n",
    "    # Label the results\n",
    "    ## (top, right, bottom, left): tọa độ 4 điểm của bounding boxes\n",
    "    for name, (top, right, bottom, left) in predictions:\n",
    "        if not name:\n",
    "            continue\n",
    "        \n",
    "        ## Tăng kích thước của bounding boxes\n",
    "        #top, right, bottom, left = top - 10, right + 10, bottom + 10, left - 10\n",
    "        # Draw a box around the face\n",
    "        ## cv2.rectangle(image, tọa_độ_đỉnh, tọa_độ_đỉnh_đối_diện, color, thickness)\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom + 25), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        ## Hiển thị chữ trong hình ảnh\n",
    "        ## cv2.putText(image, text, bottom-left_corner_of_text, font_style, font_scale, color, thickness)\n",
    "        cv2.putText(frame, name, (left + 6, bottom + 19), font, 0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    # Write the resulting image to the output video file\n",
    "    print(\"Writing frame {} / {}\".format(frame_number, length))\n",
    "    output_movie.write(frame)\n",
    "\n",
    "# All done!\n",
    "## close video file\n",
    "input_movie.release()\n",
    "## Đóng tất cả các cửa sổ đã tạo\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
